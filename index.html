<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Face-styled Diffusion Model for Text-to-Speech.">
  <meta name="keywords" content="Face-TTS, TTS, Audiovisual learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CAER (ICCV 2019)</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/NAVER_AI_Symbol_FullColor.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>

  <!-- Add icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <style>
  .btn {
    background-color: DodgerBlue;
    border: none;
    color: white;
    padding: 12px 30px;
    cursor: pointer;
    font-size: 20px;
  }

  /* Darker background on mouse-over */
  .btn:hover {
    background-color: RoyalBlue;
  }

  #topText {
    color: black;
    font-size: 20px;
    text-align: center;
    width: 100%;
  }

  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://lee-jiyoung.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Context-Aware Emotion Recognition Networks (ICCV 2019)</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://lee-jiyoung.github.io">Jiyoung Lee</a>,</span>
            <span class="author-block">
              <a href="https://cvlab.korea.ac.kr/members/faculty">Seungryong Kim</a>,
            <span class="author-block">
              <a href="https://cvlab.kau.ac.kr/home">Sunok Kim</a>,</span>
            <span class="author-block">
              <a href="https://soowhanchung.github.io">Jungin Park</a>,
            <span class="author-block">
              <a href="https://diml.yonsei.ac.kr/professor/">Kwanghoon Sohn</a>
            </span>
          </div>


          <div class="is-size-5 publication-authors">
            (Work done at all authors in Yonsei University)
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/1908.05913"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        CAER benchmark contains more than 13,000 annotated videos. </br>
        (CAER-S contains 70k frame images sampled from CAER.)</br>
        You can use CAER benchmark for emotion recognition.</br>
        The videos are annotated with an extended list of 7 emotion categories.</br>
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <p id="topText">Surprise</p>
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Surprise.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <p id="topText">Sad</p>
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Sad.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <p id="topText">Happy</p>
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Happy.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <p id="topText">Fear</p>
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Fear.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <p id="topText">Anger</p>
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Anger.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <p id="topText">Neutral</p>
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/file/sample/Neutral.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to seperately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is more appropriate than existing benchmarks both qualitatively and quantitatively. 
            On several benchmarks, CAER-Net proves the effect of context for emotion recognition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Download</h2>

    <h3 class="title is-5" style="background:RoyalBlue; color:white; display:inline;">
    The CAER dataset is available to download for research purposes. </br>
    The copyright remains with the original owners of the video.
   </h3>
    
    <div class="column is-full-width">
      <p class="container">
        Special thanks to Zhicheng Zhang (Nankai University) and Iris Dominguez (UPNA) for helping us share CAER and CAER-S again.
      </p>
      
      <hr>
      <h3 class="title is-5">CAER-S</h3>
      <p>
      The size of the CAER-S dataset is approximately 13.5GB.<br>
      The benchmark is consisted of train and test folders.<br>
      You can freely configure the training and validation sets from the train folder.<br><br>
      Extract download files with the following code.<br>
        <code>
          zip -s 0 caers_split.zip --out caers.zip;
          unzip caers.zip;
        </code>
      </p>

      <p>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1TGwKprghhECNyCRJzi7XhbH4O3n0N_Xf/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER-S 1/5</button>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1NqTGKG_9i6c0QmeTb4o2xKlAZSSr1QPA/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER-S 2/5</button>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1qt_C3lqn0MESRRkyp1JvNj23u9kWwrZW/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER-S 3/5</button> <br><br>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1MHinoWJIU7EdL7Wt6tNHTM-1NtmJUFti/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER-S 4/5</button>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1fxKxU2YjvC9XLcOz4OcvU1_DMJP_kLUc/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER-S 5/5</button>
      </p>
    <!-- </div> -->

    <hr>
    <!-- <div class="column is-full-width"> -->
      <h3 class="title is-5">CAER</h3>
      <p>
        This benchmark contains more than 13K annotated dynamic videos.</br>
        The benchmark is consisted of train, validation and test folders.</br>
      </p>
      <p>
        <button class="btn" onclick="location.href='https://drive.google.com/file/d/1JsdbBkulkIOqrchyDnML2GEmuwi6E_d2/view?usp=sharing'">
          <i class="fa fa-download"></i> CAER
        </button>
      </p>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <h3 class="title is-5" style="background:RoyalBlue; color:white; display:inline;">
      Please cite the following if you make use of the dataset.
     </h3>
  
    <pre><code>@inproceedings{lee2019context,
  author    = {Lee, Jiyoung and Kim, Seungryong and Kim, Sunok and Park, Jungin and Sohn, Kwanghoonn},
  title     = {Context-aware emotion recognition networks},
  booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
  year      = {2019},
}</code></pre>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>
    <div class="column is-full-width">
      <p>
        This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370).
      </p>
    </div>
  </div>

</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lee-jiyoung" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website refers to <a href="https://nerfies.github.io">Nerfies website</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
